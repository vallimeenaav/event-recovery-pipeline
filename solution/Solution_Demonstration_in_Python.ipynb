{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DEMONSTRATION OF MY APPROACH**\n",
        "## **Overview**  \n",
        "In this notebook, I have demonstrated how **event failures (missing, duplicate, and incorrect events) would be detected and corrected** in Linqâ€™s event-driven system.\n",
        "\n",
        "ðŸ“Œ **Note:** While this is implemented in Python, in a real-world Google Cloud setup:  \n",
        "- **Pub/Sub** would handle real-time event streaming.  \n",
        "- **Cloud Functions** would validate and detect issues.  \n",
        "- **Dataflow** would perform anomaly detection and cross-system verification.  \n",
        "- **APIs (HubSpot, Stripe, Twilio)** would be used for external validation.  \n",
        "\n",
        "\n",
        "For simplicity, I have simulated these processes in Python instead of an actual cloud deployment.  \n",
        "\n",
        "\n",
        "*Flow of my solution:*\n",
        "\n",
        "\n",
        "1. Detect failures first (event missing, duplicate, or incorrect).\n",
        "2. Reprocess faulty events using external checks and reconstruction.\n",
        "3. Publish corrected events back to the system for accurate downstream processing.\n",
        "\n",
        "*Let me take you through my solution demonstration step by step:*"
      ],
      "metadata": {
        "id": "3riaDH3cAHxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Simulating Incoming Events**"
      ],
      "metadata": {
        "id": "9RavpUHlB3Vi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will simulate events arriving via Pub/Sub with a **mock event stream**.\n",
        "- Some events will be *missing* (gaps in event IDs).\n",
        "- Some will be *duplicates* (same event ID + timestamp).\n",
        "- Some will be *incorrect* (data corruption).\n"
      ],
      "metadata": {
        "id": "mob4VuAQB9ew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB0r9BSgAEFC",
        "outputId": "18f765d2-44f1-44ee-8169-17f2b23e4e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Incoming Events:\n",
            "{'event_id': 1001, 'type': 'LeadCreated', 'timestamp': '2025-02-09T12:00:00', 'valid': True}\n",
            "{'event_id': 1002, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:01:00', 'valid': True}\n",
            "{'event_id': 1002, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:01:00', 'valid': True}\n",
            "{'event_id': 1004, 'type': 'LeadCreated', 'timestamp': '2025-02-09T12:02:00', 'valid': True}\n",
            "{'event_id': 1005, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:03:00', 'valid': True}\n",
            "{'event_id': 1005, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:03:01', 'valid': True}\n",
            "{'event_id': 1006, 'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': False}\n",
            "{'event_id': 1006, 'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': False}\n",
            "{'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': True}\n",
            "{'event_id': 1007, 'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00'}\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import uuid\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Mock event stream (simulating Pub/Sub ingestion)\n",
        "event_stream = [\n",
        "    {\"event_id\":1001, \"type\":\"LeadCreated\", \"timestamp\":\"2025-02-09T12:00:00\", \"valid\":True},\n",
        "    {\"event_id\":1002, \"type\":\"CRM_Sync\", \"timestamp\":\"2025-02-09T12:01:00\", \"valid\":True},\n",
        "    {\"event_id\":1002, \"type\":\"CRM_Sync\", \"timestamp\":\"2025-02-09T12:01:00\", \"valid\":True},\n",
        "    {\"event_id\":1004, \"type\":\"LeadCreated\", \"timestamp\":\"2025-02-09T12:02:00\", \"valid\":True},  # Missing 1003\n",
        "    {\"event_id\":1005, \"type\":\"CRM_Sync\", \"timestamp\":\"2025-02-09T12:03:00\", \"valid\":True},\n",
        "    {\"event_id\":1005, \"type\":\"CRM_Sync\", \"timestamp\":\"2025-02-09T12:03:01\", \"valid\":True},\n",
        "    {\"event_id\":1006, \"type\":\"Transaction\", \"timestamp\":\"2025-02-09T12:04:00\", \"valid\":False},\n",
        "    {\"event_id\":1006, \"type\":\"Transaction\", \"timestamp\":\"2025-02-09T12:04:00\", \"valid\":False},  # Incorrect data (assumption) and Duplicate Event\n",
        "    {\"type\":\"Transaction\", \"timestamp\":\"2025-02-09T12:04:00\", \"valid\":True}, # No event ID in this event\n",
        "    {\"event_id\":1007, \"type\":\"Transaction\", \"timestamp\":\"2025-02-09T12:04:00\"},\n",
        "]\n",
        "\n",
        "# Print the incoming event stream\n",
        "print(\"Incoming Events:\")\n",
        "for event in event_stream:\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note:* In a real cloud system, incorrect events would be identified through cross-system validation (e.g., comparing Linq's data with HubSpot, Stripe, or Twilio APIs) as I've mentioned in my approach. However, since this is a simplified  demonstration, I assume an event is incorrect if `\"valid\": False`.  \n"
      ],
      "metadata": {
        "id": "qFyvnt7AFzHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Performing a Validation Step before proceeding**"
      ],
      "metadata": {
        "id": "IcVkGmTUjil_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def validate_event_format(event):\n",
        "    \"\"\"\n",
        "    Validates the format of an event.\n",
        "\n",
        "    Checks:\n",
        "    - `event_id` is an integer (if present).\n",
        "    - `type` is a non-empty string.\n",
        "    - `timestamp` is in valid ISO 8601 format.\n",
        "    - `valid` is a boolean.\n",
        "    \"\"\"\n",
        "    # ISO 8601 Timestamp Pattern (strict format: YYYY-MM-DDTHH:MM:SS)\n",
        "    timestamp_pattern = r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}$\"\n",
        "\n",
        "    # Check for missing fields\n",
        "    if \"type\" not in event or not isinstance(event[\"type\"], str):\n",
        "        return False\n",
        "    if \"timestamp\" not in event or not re.match(timestamp_pattern, event[\"timestamp\"]):\n",
        "        return False\n",
        "    if \"valid\" not in event or not isinstance(event[\"valid\"], bool):\n",
        "        return False\n",
        "    if \"event_id\" in event and not isinstance(event[\"event_id\"], int):\n",
        "        return False  # If event_id exists, it must be an integer\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "-yZIJX4-joFH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_event_format(event):\n",
        "    \"\"\"\n",
        "    Validates and attempts to correct the format of an event.\n",
        "\n",
        "    - If `event_id` or `timestamp` is missing, sends the event to DLQ.\n",
        "    - If `valid` is missing, assumes True.\n",
        "    - If `type` is missing or empty, sends to DLQ.\n",
        "    \"\"\"\n",
        "    dlq_events = []  # Events that need manual intervention\n",
        "\n",
        "    # Send to DLQ if `event_id` is missing\n",
        "    if \"event_id\" not in event:\n",
        "        dlq_events.append(event)\n",
        "        return None, dlq_events  # Mark for manual review\n",
        "\n",
        "    # Send to DLQ if `timestamp` is missing or invalid\n",
        "    try:\n",
        "        datetime.strptime(event[\"timestamp\"], \"%Y-%m-%dT%H:%M:%S\")  # Check if valid\n",
        "    except (KeyError, ValueError):\n",
        "        dlq_events.append(event)\n",
        "        return None, dlq_events  # Mark for manual review\n",
        "\n",
        "    # Fix missing `valid` field (this is minor and can be assumed True)\n",
        "    if \"valid\" not in event:\n",
        "        event[\"valid\"] = True\n",
        "\n",
        "    # Send to DLQ if `type` is missing or empty\n",
        "    if \"type\" not in event or not isinstance(event[\"type\"], str) or event[\"type\"].strip() == \"\":\n",
        "        dlq_events.append(event)\n",
        "        return None, dlq_events  # Mark for manual review\n",
        "\n",
        "    return event, dlq_events"
      ],
      "metadata": {
        "id": "FQOM_SVPjz8V"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validated_events = []\n",
        "dlq_events = []\n",
        "\n",
        "for event in event_stream:\n",
        "    if validate_event_format(event):\n",
        "        validated_events.append(event)\n",
        "    else:\n",
        "        corrected_event, dlq_fix = correct_event_format(event)\n",
        "        if corrected_event:\n",
        "            validated_events.append(corrected_event)  # Use corrected event\n",
        "        dlq_events.extend(dlq_fix)  # Send unfixable events to DLQ\n",
        "\n",
        "# Replace `event_stream` with only valid events\n",
        "event_stream = validated_events"
      ],
      "metadata": {
        "id": "K7l60Xktj5F_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event_stream"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ18FM4vkJSe",
        "outputId": "33338462-a359-4f8f-f355-32f6d3f0de48"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'event_id': 1001,\n",
              "  'type': 'LeadCreated',\n",
              "  'timestamp': '2025-02-09T12:00:00',\n",
              "  'valid': True},\n",
              " {'event_id': 1002,\n",
              "  'type': 'CRM_Sync',\n",
              "  'timestamp': '2025-02-09T12:01:00',\n",
              "  'valid': True},\n",
              " {'event_id': 1002,\n",
              "  'type': 'CRM_Sync',\n",
              "  'timestamp': '2025-02-09T12:01:00',\n",
              "  'valid': True},\n",
              " {'event_id': 1004,\n",
              "  'type': 'LeadCreated',\n",
              "  'timestamp': '2025-02-09T12:02:00',\n",
              "  'valid': True},\n",
              " {'event_id': 1005,\n",
              "  'type': 'CRM_Sync',\n",
              "  'timestamp': '2025-02-09T12:03:00',\n",
              "  'valid': True},\n",
              " {'event_id': 1005,\n",
              "  'type': 'CRM_Sync',\n",
              "  'timestamp': '2025-02-09T12:03:01',\n",
              "  'valid': True},\n",
              " {'event_id': 1006,\n",
              "  'type': 'Transaction',\n",
              "  'timestamp': '2025-02-09T12:04:00',\n",
              "  'valid': False},\n",
              " {'event_id': 1006,\n",
              "  'type': 'Transaction',\n",
              "  'timestamp': '2025-02-09T12:04:00',\n",
              "  'valid': False},\n",
              " {'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': True},\n",
              " {'event_id': 1007,\n",
              "  'type': 'Transaction',\n",
              "  'timestamp': '2025-02-09T12:04:00',\n",
              "  'valid': True}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Detecting Issues**"
      ],
      "metadata": {
        "id": "PkYLRw4SC3TF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I will implement **detection logic** for:  \n",
        "1. Missing Events: Checking if event IDs skip a number.  \n",
        "2. Duplicate Events: Identifying duplicate timestamps.  \n",
        "3. Incorrect Events: Finding corrupted/missing data."
      ],
      "metadata": {
        "id": "2ill9pLmC86c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to detect missing ID's of events\n",
        "def detect_missing_events(events):\n",
        "    \"\"\"\n",
        "    This function detects missing event IDs by checking sequential order.\n",
        "\n",
        "    Logic:\n",
        "    - Events should have sequential IDs (e.g., 1001, 1002, 1003, 1004).\n",
        "    - If an event ID is skipped (e.g., 1001 â†’ 1002 â†’ 1004), it means 1003 is missing.\n",
        "    - This function identifies such gaps and returns a list of missing event IDs.\n",
        "    \"\"\"\n",
        "    # Filter events with 'event_id' before sorting\n",
        "    events_with_id = [event for event in events if \"event_id\" in event]\n",
        "    sorted_events = sorted(events_with_id, key=lambda x: x[\"event_id\"])\n",
        "    event_ids = {event[\"event_id\"] for event in sorted_events}\n",
        "    missing_events=[]\n",
        "\n",
        "    # Iterating through the sorted list to check for gaps in thr incoming stream\n",
        "    for i in range(min(event_ids), max(event_ids)):\n",
        "        if i not in event_ids:\n",
        "            missing_events.append(i)\n",
        "\n",
        "    return missing_events\n",
        "\n",
        "def detect_duplicate_events(events):\n",
        "    \"\"\"\n",
        "    This function identifies duplicate events by checking if the same 'event ID & timestamp' (we're assuming this to be a\n",
        "    unique combination for an incoming event) occur more than once.\n",
        "\n",
        "    Logic:\n",
        "    - If an event has the same `event_id` and `timestamp` as another event, it's a duplicate.\n",
        "    - The function uses a set to track seen event keys and then detect duplicates.\n",
        "    - This function now checks if 'event_id' is present in the event before creating the key.\n",
        "    - If 'event_id' is missing, the event is considered unique and not a duplicate - will be addressed later.\n",
        "    \"\"\"\n",
        "    seen = set() # Unique event identifiers (event_id+timestamp)\n",
        "    duplicates=[] # List to store duplicate events\n",
        "\n",
        "    for event in events:\n",
        "        if \"event_id\" in event:  # Check if 'event_id' exists in the event\n",
        "            key = (event[\"event_id\"], event[\"timestamp\"]) # Storing the unique event key\n",
        "            if key in seen:\n",
        "                duplicates.append(event)\n",
        "            else:\n",
        "                seen.add(key)\n",
        "\n",
        "    return duplicates\n",
        "\n",
        "# Defining a function to detect incorrect events\n",
        "def detect_incorrect_events(events):\n",
        "    \"\"\"\n",
        "    This function flags events that have incorrect or corrupted data.\n",
        "\n",
        "    Logic:\n",
        "    - If an event has `valid=False`, it is considered incorrect (e.g., missing fields, wrong values).\n",
        "    - The function filters out such events and returns a list of incorrect ones.\n",
        "    \"\"\"\n",
        "    return [event for event in events if not event[\"valid\"] or \"event_id\" not in event]\n",
        "\n",
        "# Run detection\n",
        "missing=detect_missing_events(event_stream)\n",
        "duplicates=detect_duplicate_events(event_stream)\n",
        "incorrect=detect_incorrect_events(event_stream)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n Missing Events:\", missing)\n",
        "print(\"Duplicate Events:\", duplicates)\n",
        "print(\"Incorrect Events:\", incorrect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z_UbbvwC50r",
        "outputId": "1e5d7cfe-c418-4f18-82b5-200e36872cbf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Missing Events: [1003]\n",
            "Duplicate Events: [{'event_id': 1002, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:01:00', 'valid': True}, {'event_id': 1006, 'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': False}]\n",
            "Incorrect Events: [{'event_id': 1006, 'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': False}, {'event_id': 1006, 'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': False}, {'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': True}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Reconstructing & Fixing Events**"
      ],
      "metadata": {
        "id": "kKWND3tHIsN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that I've detected issues, I will:  \n",
        "1. Recreate missing events using inference.  \n",
        "2. Remove duplicate events to prevent double processing.  \n",
        "3. Fix incorrect data using API-based corrections (simulated)."
      ],
      "metadata": {
        "id": "hcadcYmfIxx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_missing_events(existing_events):\n",
        "    \"\"\"\n",
        "    This function reconstructs missing events dynamically using `detect_missing_events()`.\n",
        "\n",
        "    Steps:\n",
        "    - First, it detect missing events using Step 2 function.\n",
        "    - Then, estimate timestamps from neighboring events (previous & next).\n",
        "    - Assign event type dynamically based on event frequency (this is an assumption for the sake of this demonstration).\n",
        "    - Return the reconstructed events.\n",
        "\n",
        "    Again, for production, we would factor in more business logic as I've mentioned in my approach\n",
        "    \"\"\"\n",
        "    missing_event_ids = detect_missing_events(existing_events)\n",
        "    reconstructed_events=[]\n",
        "\n",
        "    # Sort events for timestamp estimation\n",
        "    # Filter events with 'event_id' before sorting to avoid KeyError\n",
        "    events_with_id = [event for event in existing_events if \"event_id\" in event]\n",
        "    sorted_events = sorted(events_with_id, key=lambda x: x[\"event_id\"])\n",
        "\n",
        "    for event_id in missing_event_ids:\n",
        "        # Find closest previous and next events\n",
        "        prev_event = None\n",
        "        next_event = None\n",
        "        for event in sorted_events:\n",
        "            if event[\"event_id\"] < event_id:\n",
        "                prev_event = event  # Last event before missing one\n",
        "            elif event[\"event_id\"] > event_id and next_event is None:\n",
        "                next_event = event  # First event after missing one\n",
        "                break\n",
        "\n",
        "        # Estimate timestamp based on neighboring events\n",
        "        # Logic:\n",
        "        \"\"\"\n",
        "        If a previous event (prev_event) and a next event (next_event) exist, the missing eventâ€™s timestamp is calculated as the midpoint between them.\n",
        "        If only a previous event exists, the missing event's timestamp is placed shortly after it (+30 seconds).\n",
        "        If only a next event exists, the missing event's timestamp is placed shortly before it (-30 seconds).\n",
        "        If no reference points exist, the function assigns it to the current time (datetime.utcnow()) as a default.\n",
        "        \"\"\"\n",
        "        if prev_event and next_event:\n",
        "            estimated_timestamp = datetime.strptime(prev_event[\"timestamp\"], \"%Y-%m-%dT%H:%M:%S\") + (\n",
        "                datetime.strptime(next_event[\"timestamp\"], \"%Y-%m-%dT%H:%M:%S\") - datetime.strptime(prev_event[\"timestamp\"], \"%Y-%m-%dT%H:%M:%S\")\n",
        "            ) / 2\n",
        "        elif prev_event:\n",
        "            estimated_timestamp = datetime.strptime(prev_event[\"timestamp\"], \"%Y-%m-%dT%H:%M:%S\") + timedelta(seconds=30)\n",
        "        elif next_event:\n",
        "            estimated_timestamp = datetime.strptime(next_event[\"timestamp\"], \"%Y-%m-%dT%H:%M:%S\") - timedelta(seconds=30)\n",
        "        else:\n",
        "            estimated_timestamp = datetime.utcnow()\n",
        "\n",
        "        # Assign event type (most frequent type in event stream)\n",
        "        event_types = [event[\"type\"] for event in existing_events]\n",
        "        assumed_event_type = max(set(event_types), key=event_types.count)\n",
        "\n",
        "        # Create reconstructed event\n",
        "        reconstructed_events.append({\"event_id\": event_id, \"type\": assumed_event_type, \"timestamp\": estimated_timestamp.isoformat(), \"valid\": True})\n",
        "\n",
        "    return reconstructed_events\n",
        "\n",
        "\n",
        "def remove_duplicates(events):\n",
        "    \"\"\"\n",
        "    This function removes duplicate events while keeping one valid instance.\n",
        "\n",
        "    Steps:\n",
        "    - Identify duplicate events using `detect_duplicate_events()`.\n",
        "    - Keep one valid instance of each duplicate.\n",
        "    \"\"\"\n",
        "    unique_events = []\n",
        "    seen = set()\n",
        "\n",
        "    for event in events:\n",
        "        if \"event_id\" in event:  # Ensure event_id exists\n",
        "            key = (event[\"event_id\"], event[\"timestamp\"])\n",
        "            if key not in seen:\n",
        "                unique_events.append(event)\n",
        "                seen.add(key)\n",
        "        else:\n",
        "            # If no event_id, just add as-is (handled separately in DLQ logic)\n",
        "            unique_events.append(event)\n",
        "\n",
        "    return unique_events\n",
        "\n",
        "\n",
        "def fix_incorrect_events(events):\n",
        "    \"\"\"\n",
        "    This function fixes incorrect events (sending to DLQ for manual review).\n",
        "\n",
        "    Steps:\n",
        "    - Identify incorrect events using `detect_incorrect_events()`.\n",
        "    - If an event is missing critical fields, send it to DLQ.\n",
        "    - If valid=False, send it to DLQ instead of fixing it.\n",
        "    \"\"\"\n",
        "    incorrect_events = detect_incorrect_events(events)\n",
        "    corrected_events = []\n",
        "    dlq_events = []  # Events needing manual review\n",
        "\n",
        "    for event in events:\n",
        "        if event in incorrect_events:\n",
        "            # Case 1: Critical fields missing -> Send to DLQ\n",
        "            if \"event_id\" not in event or not event[\"valid\"]:  # If valid=False, send to DLQ\n",
        "                dlq_events.append(event)\n",
        "                continue  # Skip adding this event to corrected list\n",
        "\n",
        "        # Otherwise, keep the event\n",
        "        corrected_events.append(event)\n",
        "\n",
        "    return corrected_events, dlq_events\n",
        "\n",
        "\n",
        "reconstructed_events=reconstruct_missing_events(event_stream)  # Fix missing events\n",
        "cleaned_event_stream=remove_duplicates(event_stream)  # Remove duplicate events\n",
        "fixed_events, dlq_events=fix_incorrect_events(cleaned_event_stream)  # Fix incorrect events\n",
        "\n",
        "final_event_stream = fixed_events+reconstructed_events  # Final list for publishing\n",
        "\n",
        "# Send DLQ Events for Manual Review (Combine previous DLQ + New DLQ)\n",
        "dlq_events.extend(dlq_fix)\n",
        "\n",
        "print(\"\\n Reconstructed Missing Events:\", reconstructed_events)\n",
        "print(\"Final Cleaned & Fixed Event Stream:\", final_event_stream)\n",
        "print(\"Events Sent to DLQ (Manual Review Required):\", dlq_events)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DqqetqNI_Qm",
        "outputId": "46776bab-2528-46d9-9b6f-75d23e556f53"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Reconstructed Missing Events: [{'event_id': 1003, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:01:30', 'valid': True}]\n",
            "Final Cleaned & Fixed Event Stream: [{'event_id': 1001, 'type': 'LeadCreated', 'timestamp': '2025-02-09T12:00:00', 'valid': True}, {'event_id': 1002, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:01:00', 'valid': True}, {'event_id': 1004, 'type': 'LeadCreated', 'timestamp': '2025-02-09T12:02:00', 'valid': True}, {'event_id': 1005, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:03:00', 'valid': True}, {'event_id': 1005, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:03:01', 'valid': True}, {'event_id': 1007, 'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': True}, {'event_id': 1003, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:01:30', 'valid': True}]\n",
            "Events Sent to DLQ (Manual Review Required): [{'event_id': 1006, 'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': False}, {'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': True}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Step 4: Publishing Fixed Events**"
      ],
      "metadata": {
        "id": "6gNKcw6fJBOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Finally, I will send corrected events back to the system (simulated Pub/Sub).\n"
      ],
      "metadata": {
        "id": "7AhlrdnNI_oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def publish_to_pubsub(events, topic_name=\"cleaned-events\"):\n",
        "    \"\"\"\n",
        "    This function simulates publishing corrected events back to Pub/Sub.\n",
        "\n",
        "    Steps:\n",
        "    - Iterates over all cleaned events.\n",
        "    - Publishes them to a mock Pub/Sub topic.\n",
        "    - Logs event IDs for tracking.\n",
        "    \"\"\"\n",
        "    print(f\"\\n Publishing {len(events)} cleaned events to Pub/Sub topic: {topic_name}\")\n",
        "    for event in events:\n",
        "        print(f\"Published: {event}\")\n",
        "\n",
        "\n",
        "def send_to_dlq(events, dlq_topic_name=\"dead-letter-queue\"):\n",
        "    \"\"\"\n",
        "    This function simulates sending events to Dead Letter Queue (DLQ) for manual review.\n",
        "\n",
        "    Steps:\n",
        "    - Iterates over events that couldn't be fixed.\n",
        "    - Publishes them to a separate DLQ for human intervention.\n",
        "    \"\"\"\n",
        "    print(f\"\\n Sending {len(events)} problematic events to DLQ: {dlq_topic_name}\")\n",
        "    for event in events:\n",
        "        print(f\"DLQ Event: {event}\")\n",
        "\n",
        "\n",
        "# Publish Cleaned Data\n",
        "publish_to_pubsub(final_event_stream)\n",
        "\n",
        "# Send Problematic Events to DLQ\n",
        "send_to_dlq(dlq_events)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WZzFzb5Qqqm",
        "outputId": "87887863-362f-443f-ae39-8e43c8dadcea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Publishing 7 cleaned events to Pub/Sub topic: cleaned-events\n",
            "Published: {'event_id': 1001, 'type': 'LeadCreated', 'timestamp': '2025-02-09T12:00:00', 'valid': True}\n",
            "Published: {'event_id': 1002, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:01:00', 'valid': True}\n",
            "Published: {'event_id': 1004, 'type': 'LeadCreated', 'timestamp': '2025-02-09T12:02:00', 'valid': True}\n",
            "Published: {'event_id': 1005, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:03:00', 'valid': True}\n",
            "Published: {'event_id': 1005, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:03:01', 'valid': True}\n",
            "Published: {'event_id': 1007, 'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': True}\n",
            "Published: {'event_id': 1003, 'type': 'CRM_Sync', 'timestamp': '2025-02-09T12:01:30', 'valid': True}\n",
            "\n",
            " Sending 2 problematic events to DLQ: dead-letter-queue\n",
            "DLQ Event: {'event_id': 1006, 'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': False}\n",
            "DLQ Event: {'type': 'Transaction', 'timestamp': '2025-02-09T12:04:00', 'valid': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How This Would Work in Google Cloud (GCP)\n",
        "While I've demonstrated the logic in Python here, in a real production setup at Linq, this solution would be deployed as follows:\n",
        "\n",
        "### **Event Streaming with Pub/Sub**\n",
        "- Events (e.g., `\"Business Card Scanned\"`, `\"CRM Sync\"`, `\"Revenue Transaction\"`) are **published to Google Pub/Sub** as messages.\n",
        "- The system listens for new events in real time.\n",
        "\n",
        "### **Event Validation Using Cloud Functions**\n",
        "- **A Google Cloud Function** is triggered for every new event.\n",
        "- It **validates event format** (checks required fields, timestamps, etc.).\n",
        "- If the event is **invalid**, it is sent to a **Dead Letter Queue (DLQ)** for review.\n",
        "\n",
        "### **Detecting & Handling Event Issues**\n",
        "#### Missing Events\n",
        "- A **Cloud Function checks for ID gaps** in sequentially processed events.\n",
        "- **If an event is missing**, a reprocessing request is **triggered via Pub/Sub**.\n",
        "\n",
        "#### Duplicate Events\n",
        "- **UUID-based deduplication** ensures that duplicate events donâ€™t get reprocessed.\n",
        "- **If a duplicate is detected**, it is sent to a **DLQ** instead of being stored.\n",
        "\n",
        "#### Incorrectly Processed Events\n",
        "- **Cloud Dataflow cross-checks event data** with external APIs (HubSpot, Stripe, Twilio).\n",
        "- If an event **doesnâ€™t match expected values**, it is **flagged for recalculation**.\n",
        "\n",
        "### **Reprocessing & Correction**\n",
        "- **A Cloud Function reconstructs missing events** using inferred properties.\n",
        "- **A Cloud Run service** queries APIs to **fix incorrect events dynamically**.\n",
        "- **Corrected events are published back to Pub/Sub** to be reprocessed normally.\n",
        "\n",
        "### **Storing & Using the Corrected Events**\n",
        "- Once an event is **validated and correctly processed**, it flows **into Linqâ€™s database/CRM system**.\n",
        "- **AI-driven workflows** (like follow-ups and CRM syncs) run **only on clean, validated events**.\n",
        "\n",
        "By implementing this architecture, Linq can **ensure high data reliability** while processing millions of events per hour.\n"
      ],
      "metadata": {
        "id": "Mq_-net7evx4"
      }
    }
  ]
}